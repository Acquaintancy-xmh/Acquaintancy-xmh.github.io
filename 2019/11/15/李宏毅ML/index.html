<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>李宏毅ML | ACQUAINT</title><meta name="description" content="李宏毅ML"><meta name="keywords" content="Machine Learning"><meta name="author" content="Acquaintancy"><meta name="copyright" content="Acquaintancy"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="李宏毅ML"><meta name="twitter:description" content="李宏毅ML"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png"><meta property="og:type" content="article"><meta property="og:title" content="李宏毅ML"><meta property="og:url" content="http://yoursite.com/2019/11/15/%E6%9D%8E%E5%AE%8F%E6%AF%85ML/"><meta property="og:site_name" content="ACQUAINT"><meta property="og:description" content="李宏毅ML"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://yoursite.com/2019/11/15/%E6%9D%8E%E5%AE%8F%E6%AF%85ML/"><link rel="next" title="Hello World" href="http://yoursite.com/2019/11/15/hello-world/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: undefined,
  medium_zoom: 'false',
  Snackbar: undefined
  
}</script></head><body><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">ACQUAINT</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span><span class="pull_right" id="search_button"></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/Photo/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">2</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">1</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture1-Regression"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">ML Lecture1 Regression</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture2-Where-does-the-error-come-from"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">ML Lecture2 Where does the error come from?</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture3-Gradient-Descent"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">ML Lecture3 Gradient Descent</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Strochastic-Gradient-Descent"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">Strochastic Gradient Descent</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Feature-Scaling"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">Feature Scaling</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture4-Classification"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">ML Lecture4 Classification</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture5-Logistic-Regression"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">ML Lecture5 Logistic Regression</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture9-Tips-for-Training-DNN"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">ML Lecture9 Tips for Training DNN</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-lecture10-Convolutional-Neural-Network"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text">ML lecture10 Convolutional Neural Network</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture11-Why-Deep？"><span class="toc_mobile_items-number">8.</span> <span class="toc_mobile_items-text">ML Lecture11 Why Deep？</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture12-Semi-supervised"><span class="toc_mobile_items-number">9.</span> <span class="toc_mobile_items-text">ML Lecture12 Semi-supervised</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture13-Unsupervised-Learning-Linear-Methods"><span class="toc_mobile_items-number">10.</span> <span class="toc_mobile_items-text">ML Lecture13 Unsupervised Learning-Linear Methods</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture14-Unsupervised-Learning-Word-Embedding"><span class="toc_mobile_items-number">11.</span> <span class="toc_mobile_items-text">ML Lecture14 Unsupervised Learning-Word Embedding</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture15-Unsupervised-Learning-Neighbor-Embedding"><span class="toc_mobile_items-number">12.</span> <span class="toc_mobile_items-text">???ML Lecture15 Unsupervised Learning - Neighbor Embedding</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture16-Unsupervised-Learning-Auto-encoder"><span class="toc_mobile_items-number">13.</span> <span class="toc_mobile_items-text">ML Lecture16 Unsupervised Learning - Auto-encoder</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture17-Unsupervised-Learning-Deep-Generative-Model"><span class="toc_mobile_items-number">14.</span> <span class="toc_mobile_items-text">???ML Lecture17 Unsupervised Learning - Deep Generative Model</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture19-Transfer-Learning"><span class="toc_mobile_items-number">15.</span> <span class="toc_mobile_items-text">???ML Lecture19 Transfer Learning</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture20-Support-Vector-Machine-SVM"><span class="toc_mobile_items-number">16.</span> <span class="toc_mobile_items-text">???ML Lecture20 Support Vector Machine(SVM)</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture21-Recurrent-Neural-Network"><span class="toc_mobile_items-number">17.</span> <span class="toc_mobile_items-text">ML Lecture21 Recurrent Neural Network</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#ML-Lecture32-Deep-Reinforcement-Learning"><span class="toc_mobile_items-number">18.</span> <span class="toc_mobile_items-text">ML Lecture32 Deep Reinforcement Learning</span></a></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture1-Regression"><span class="toc-number">1.</span> <span class="toc-text">ML Lecture1 Regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture2-Where-does-the-error-come-from"><span class="toc-number">2.</span> <span class="toc-text">ML Lecture2 Where does the error come from?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture3-Gradient-Descent"><span class="toc-number">3.</span> <span class="toc-text">ML Lecture3 Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Strochastic-Gradient-Descent"><span class="toc-number">3.1.</span> <span class="toc-text">Strochastic Gradient Descent</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Feature-Scaling"><span class="toc-number">3.2.</span> <span class="toc-text">Feature Scaling</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture4-Classification"><span class="toc-number">4.</span> <span class="toc-text">ML Lecture4 Classification</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture5-Logistic-Regression"><span class="toc-number">5.</span> <span class="toc-text">ML Lecture5 Logistic Regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture9-Tips-for-Training-DNN"><span class="toc-number">6.</span> <span class="toc-text">ML Lecture9 Tips for Training DNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-lecture10-Convolutional-Neural-Network"><span class="toc-number">7.</span> <span class="toc-text">ML lecture10 Convolutional Neural Network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture11-Why-Deep？"><span class="toc-number">8.</span> <span class="toc-text">ML Lecture11 Why Deep？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture12-Semi-supervised"><span class="toc-number">9.</span> <span class="toc-text">ML Lecture12 Semi-supervised</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture13-Unsupervised-Learning-Linear-Methods"><span class="toc-number">10.</span> <span class="toc-text">ML Lecture13 Unsupervised Learning-Linear Methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture14-Unsupervised-Learning-Word-Embedding"><span class="toc-number">11.</span> <span class="toc-text">ML Lecture14 Unsupervised Learning-Word Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture15-Unsupervised-Learning-Neighbor-Embedding"><span class="toc-number">12.</span> <span class="toc-text">???ML Lecture15 Unsupervised Learning - Neighbor Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture16-Unsupervised-Learning-Auto-encoder"><span class="toc-number">13.</span> <span class="toc-text">ML Lecture16 Unsupervised Learning - Auto-encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture17-Unsupervised-Learning-Deep-Generative-Model"><span class="toc-number">14.</span> <span class="toc-text">???ML Lecture17 Unsupervised Learning - Deep Generative Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture19-Transfer-Learning"><span class="toc-number">15.</span> <span class="toc-text">???ML Lecture19 Transfer Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture20-Support-Vector-Machine-SVM"><span class="toc-number">16.</span> <span class="toc-text">???ML Lecture20 Support Vector Machine(SVM)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture21-Recurrent-Neural-Network"><span class="toc-number">17.</span> <span class="toc-text">ML Lecture21 Recurrent Neural Network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ML-Lecture32-Deep-Reinforcement-Learning"><span class="toc-number">18.</span> <span class="toc-text">ML Lecture32 Deep Reinforcement Learning</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png)"><div id="post-info"><div id="post-title"><div class="posttitle">李宏毅ML</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2019-11-15<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2019-11-15</time><div class="post-meta-wordcount"><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>Post View: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><p>![](/Users/xmh_mac/Desktop/屏幕快照 2019-08-20 20.12.49.png)</p>
<h3 id="ML-Lecture1-Regression"><a href="#ML-Lecture1-Regression" class="headerlink" title="ML Lecture1 Regression"></a>ML Lecture1 Regression</h3><ol>
<li>案例研究–回归</li>
</ol>
<h3 id="ML-Lecture2-Where-does-the-error-come-from"><a href="#ML-Lecture2-Where-does-the-error-come-from" class="headerlink" title="ML Lecture2 Where does the error come from?"></a>ML Lecture2 Where does the error come from?</h3><ol>
<li>variance和bias是针对model来说的（针对function set来说）</li>
<li>error来自于variance和bias：variance是每次的预测值和期望值的差，bias是预测值的平均值与期望值的差（即是否“瞄准”）</li>
<li>越复杂的model，variance越大，因为越复杂的model，受到不同data的影响越大。</li>
<li>越复杂的model，bias越小，因为越复杂的function set，越可能包含最好的model。</li>
<li>如果error来自于variance很大，即为Overfitting。</li>
<li>如果error来自于bias很大，即为Underfitting。</li>
<li>If your model cannot even fit the training examples,then you have large bias. –Underfitting</li>
<li>If you can fit the training data,but large error on testing data,then you probably have large variance.–Overfitting</li>
<li>For bias,redesign your model:<ol>
<li>Add more features as input</li>
<li>A more conplex model</li>
</ol>
</li>
<li>For variance:<ol>
<li>More data</li>
<li>Regularization</li>
</ol>
</li>
<li>Select a model that balances two kinds of error to minimize total error.</li>
</ol>
<h3 id="ML-Lecture3-Gradient-Descent"><a href="#ML-Lecture3-Gradient-Descent" class="headerlink" title="ML Lecture3 Gradient Descent"></a>ML Lecture3 Gradient Descent</h3><p>####Learning Rate</p>
<ol>
<li><p>可以通过visualize参数对loss的影响的示意图，来发现是否使用了合适的learning rate</p>
<p>![](/Users/xmh_mac/Desktop/屏幕快照 2019-08-10 21.37.53.png)</p>
</li>
<li><p>Reduce the learning rate by some factor every few epochs</p>
</li>
<li><p>最好的情况是每个参数给不同的learning rate</p>
</li>
<li><p>Adagrad:Divide the learning rate of each parameter by the <strong>root mean square</strong> of its previous derivatives.</p>
<p>为了更直观得察觉出“反差”，强调这种反差的效果，使得更敏感。 </p>
</li>
</ol>
<p>   ![](/Users/xmh_mac/Desktop/屏幕快照 2019-08-10 21.45.46.png)</p>
<p>![](/Users/xmh_mac/Desktop/屏幕快照 2019-08-10 21.48.07.png)</p>
<p><strong>Use first derivative to estimate second derivative</strong></p>
<p>![](/Users/xmh_mac/Desktop/屏幕快照 2019-08-10 22.02.53.png)</p>
<ol start="5">
<li><strong>Adam?</strong></li>
</ol>
<h4 id="Strochastic-Gradient-Descent"><a href="#Strochastic-Gradient-Descent" class="headerlink" title="Strochastic Gradient Descent"></a>Strochastic Gradient Descent</h4><ol>
<li><p>Loss is the summation over all training examples.</p>
</li>
<li><p>Stochastic Gradient Descent:Pick an example. </p>
</li>
<li><p>以多次的update，更简单的计算，虽然可能会走弯路，但是会获得更好的进展和更准确的方向。</p>
</li>
</ol>
<h4 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h4><ol>
<li>为了防止某个feature造成的影响特别大，影响其他feature</li>
</ol>
<h3 id="ML-Lecture4-Classification"><a href="#ML-Lecture4-Classification" class="headerlink" title="ML Lecture4 Classification"></a>ML Lecture4 Classification</h3><ol>
<li>如果把Classification当做Regression来做的话：<ol>
<li>regression会惩罚一些“too correct”的点，训练的效果不好</li>
<li>如果Multiple class，则会出现regression默认把class1与feature1相关联的情况。</li>
</ol>
</li>
</ol>
<h3 id="ML-Lecture5-Logistic-Regression"><a href="#ML-Lecture5-Logistic-Regression" class="headerlink" title="ML Lecture5 Logistic Regression"></a>ML Lecture5 Logistic Regression</h3><ol>
<li><p>交叉熵（Cross entropy）：衡量距离的公式</p>
<p>![image-20190813215930504](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190813215930504.png)</p>
<p>所以，Logistic Regression中的Loss Function是：交叉熵的总和，即：![image-20190813220055933](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190813220055933.png)</p>
<p>![image-20190813220111600](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190813220111600.png)</p>
<p>（在tensorflow中有实践）</p>
</li>
</ol>
<ol start="2">
<li><p>Logistic Regression 和 Linear Regression</p>
<p>![image-20190813221239140](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190813221239140.png)</p>
</li>
<li><p>Generative Model &amp; Discriminative Model（生成模型和判别模型）</p>
<p>在生成模型中，我们首先得到每一个类的条件概率（假设），以及先验概率，然后通过贝叶斯定理得到概率。哪个后验概率大，就属于哪个类。</p>
<p>在判别模型中，直接学习后验概率，然后利用决策理论判断输入数据属于哪个类，逻辑回归和感知机都属于典型的判别模型。</p>
<p>生成模型的好处：</p>
<ul>
<li><p>对概率分布进行假设，需要少量的数据集就可以完成</p>
</li>
<li><p>可以更好地排除噪声</p>
</li>
<li><p>先验概率和后验概率可以拆开考虑，使用不同的source</p>
</li>
</ul>
</li>
</ol>
<h3 id="ML-Lecture9-Tips-for-Training-DNN"><a href="#ML-Lecture9-Tips-for-Training-DNN" class="headerlink" title="ML Lecture9 Tips for Training DNN"></a>ML Lecture9 Tips for Training DNN</h3><ol>
<li><p>Do not always blame Overfitting: 如果发现一个训练后的模型在testing data上表现不好，而一个低层数的模型反而表现较好，不要马上下结论是Overfitting。<strong>要先在Training Data上测试模型，如果在training data上效果相反，有可能是Overfitting，否则，可能是其他的因素影响了模型的效果。</strong></p>
</li>
<li><p><strong>Recipe of Deep Learning: Neural Network -&gt; Good Results on Training Data? -&gt;Good Results on Testing Data?</strong>   先在training data上测试。</p>
</li>
<li><p>在Training Data上表现不好？ </p>
<ol>
<li><p>New activation function:Vanishing Gradient Problem（梯度消失 即改变input 对output影响越来越小 ）Sigmoid会导致梯度消失的发生。 </p>
<p>可用来替换的：ReLU（ReLU is a special cases of <strong>Maxout</strong>） </p>
</li>
<li><p>Momentum: 在Gradient中，加入“惯性”，由上一时刻下降的方向和本时刻的Gradient共同决定本时刻的下降方向。这种方法可以解决一部分local minima产生的影响。</p>
<p>RMSProp:针对Learning Rate的优化方案。</p>
<p>Adam = RMSProp + Momentum。</p>
</li>
</ol>
</li>
<li><p>在Testing Data上表现不好？</p>
<ol>
<li><p>Early Stopping:因为Training Data和Testing Data的distribution可能不同，所以当model在Traing data上Loss仍然在下降的时候，在Testing Data上的Loss已经上升了。Model应该停在Testing data最小的位置，所以可以在Training data中切出一个Validation Set，来模拟Testing Data。（可以得出适当的epoch数目）</p>
</li>
<li><p>Regularization:在Loss Function中加一个Regularization term（L2)。![屏幕快照 2019-08-16 下午19.31.44 下午](/Users/xmh_mac/Desktop/屏幕快照 2019-08-16 下午19.31.44 下午.png)</p>
<p>会让weight每次都减小一点，即Weight Decay。</p>
<p>过拟合的时候，拟合函数的系数往往非常大，为什么？如下图所示，过拟合，就是拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大，由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。而正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。</p>
</li>
</ol>
</li>
<li><p><strong>Dropout</strong>:   Each neuron has p% to dropout.Using the new netword for training.（有可能drop <strong>InputLayer和HiddenLayer</strong>的neuron）</p>
<p>在Training时，加入dropout时，效果可能会变差。</p>
<p>   在Testing时，不加dropout。<strong>If the dropout rate at training is p%,all the weights times(1-p)%</strong></p>
</li>
</ol>
<pre><code>![屏幕快照 2019-08-16 下午19.46.50 下午](/Users/xmh_mac/Desktop/屏幕快照 2019-08-16 下午19.46.50 下午.png).</code></pre><h3 id="ML-lecture10-Convolutional-Neural-Network"><a href="#ML-lecture10-Convolutional-Neural-Network" class="headerlink" title="ML lecture10 Convolutional Neural Network"></a>ML lecture10 Convolutional Neural Network</h3><ol>
<li><p>CNN是为了简化Neuron Network的参数。</p>
</li>
<li><p>Property：Convolution–Some patterns are much smaller than the whole image/The same patterns appear in different regions      Max Pooling–Subsampling the pixels will not change the object.</p>
<p>即：Convolution–缩小范围找到feature、忽略位置关系  Max Pooling–降维，减少计算量</p>
</li>
<li><p>Convolution层有多少个filter，output就有多少个feature map。假设第一层有25个filter，第一层output 25个feature map，如果第二层也有25个filter，那么第二层的output 也有25个feature map，只不过第二层的每个filter是一个立方体（高为25），对第一层的立方体feature map进行卷积。</p>
</li>
</ol>
<h3 id="ML-Lecture11-Why-Deep？"><a href="#ML-Lecture11-Why-Deep？" class="headerlink" title="ML Lecture11 Why Deep？"></a>ML Lecture11 Why Deep？</h3><ol>
<li><p>Fat + Short v.s. Thin + Tall ? 到底NN是不是越深越好？是。同样的neuron数量，deeper&gt;fatter</p>
</li>
<li><p>Modularization：如果某一种classifier的data很少，可以将classifier分类，分成小类，分别train，然后模块化（类似于函数）。eg.Boys with long hair – Boys + long hair.</p>
<p>所以，通过Modularization，可以将需要的data减少。</p>
</li>
<li><p>Deep Learning : 可以找到一些普通分类找不到的feature。有一些的hidden layer可以充当transformer的角色，将feature转换到更好学习的状态。 </p>
</li>
</ol>
<h3 id="ML-Lecture12-Semi-supervised"><a href="#ML-Lecture12-Semi-supervised" class="headerlink" title="ML Lecture12 Semi-supervised"></a>ML Lecture12 Semi-supervised</h3><ol>
<li><p>Why? Collecting data is easy,but collecting “labeled” data is expensive.</p>
</li>
<li><p>A set of unlabeled data,通常数量大于有label的data数量。</p>
</li>
<li><p>Transductive Learning: unlabeled data is the testing data</p>
<p>Inductive Learning: unlabeled data is not the testing data</p>
</li>
<li><p><strong>Low-density Separation</strong>: 两个class之间会有一个明显的交界处，在这个交界处density比较低</p>
<p><strong>Self-training</strong>: 先从labeled data中train出一个model；用model去label出unlabeled data的label；选出一些enlabel data加入到labeled data。重复以上的 步骤。 <strong>Regression不可用这个方法</strong></p>
<p>Entropy-based Regularization: 判断unlabeled data的label分布是不是集中的，model应使其尽量集中。Entropy:Evaluate how concentrate the distribution y is.![image-20190817135729332](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190817135729332.png)</p>
<p>右下角的L即为Loss Function，因为思路上比较像Regularization，故起名。</p>
</li>
<li><p>Cluster-then-label</p>
<p>Smoothness Assumption: “similar” x has the same y:即如果x的分布是不平均的，在一些地方很集中，如果x1和x2in a high density region，那么两个的y是一样的。 <strong>不单纯看相似度，而是看两者之间有没有间接相连的step可以连接过去</strong>。即，先分类，获得unlabel data的label，然后正常train model就可以了。</p>
<p>Graph-based Approach：网页、论文等，直接通过连接，将点连在一起绘图。连在一起的点即similar。或者可以通过K Nearest Neighbor，e-Neighborhood的方法绘图。</p>
</li>
</ol>
<h3 id="ML-Lecture13-Unsupervised-Learning-Linear-Methods"><a href="#ML-Lecture13-Unsupervised-Learning-Linear-Methods" class="headerlink" title="ML Lecture13 Unsupervised Learning-Linear Methods"></a>ML Lecture13 Unsupervised Learning-Linear Methods</h3><ol>
<li><p>Clustering: </p>
<p>K-means: ![image-20190817150211198](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190817150211198.png)</p>
<p>Hierarchical Agglomerative Clustering(HAC): 根据相似度，生成tree，然后pick a threshold，进行分类。![image-20190817150610363](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190817150610363.png)</p>
<p>两者的区别是如何决定K的数量，可以用HAC做出K，然后用K-means做分类。</p>
</li>
<li><p>Distributed representation：用一个vector来表示属于某些类型的数值，而不是简单地说属于哪一类。</p>
</li>
<li><p>Dimension Reduction：降维。以下是方法：</p>
<p>Feature selection：选择一个feature。</p>
<p><strong>Principle component analysis（PCA）</strong>：</p>
<p>Target : z=Wx   –We want the variance of z as large as possible . ![image-20190818170937699](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190818170937699.png)</p>
<p>用拉格朗日方法解W（找eigenvector）。 </p>
<p>可以用一个NN来表示PCA，即Autoencoder。–但是NN得出的w与PCA解出的w是不同的，因为少了“正交”的条件。Linear的情况，用PCA比较快，但是NN的好处是可以是deep的，即<strong>deep autoencoder</strong>。![image-20190818192531105](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190818192531105.png)</p>
<p>Weakness:Unsupervised&amp;Linear. –LDA：考虑label的降维方法。</p>
<p>PCA的weight可能是正的也可能是负的，所以可能是components的相减得到结果。</p>
<p><strong>NMF</strong>:component的参数都是正的PCA</p>
</li>
<li><p>Matrix Factorization  :  推荐算法 – LSA</p>
</li>
</ol>
<h3 id="ML-Lecture14-Unsupervised-Learning-Word-Embedding"><a href="#ML-Lecture14-Unsupervised-Learning-Word-Embedding" class="headerlink" title="ML Lecture14 Unsupervised Learning-Word Embedding"></a>ML Lecture14 Unsupervised Learning-Word Embedding</h3><ol>
<li><p>Generating Word Vector is unsupervised. 只有输入 没有输出</p>
</li>
<li><p>Machine learn the meaning of words from reading a lot of documents without supervision.   A word can be understood by its context.</p>
</li>
<li><p>Prediction-based: 把wi-1词的one-hot向量作为NN的输入，把wi词的概率作为NN的输出，即用wi-1去predict下一个词的概率。训练后的NN的第一层hidden layer的z拿出来，即为word embedding的结果。</p>
<p>如果用wi-1和wi-2来预测wi，那么要wi-1和wi-2share parameter。![image-20190817170928716](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190817170928716.png)</p>
<p>​    </p>
</li>
</ol>
<h3 id="ML-Lecture15-Unsupervised-Learning-Neighbor-Embedding"><a href="#ML-Lecture15-Unsupervised-Learning-Neighbor-Embedding" class="headerlink" title="???ML Lecture15 Unsupervised Learning - Neighbor Embedding"></a>???ML Lecture15 Unsupervised Learning - Neighbor Embedding</h3><ol>
<li>即非线性条件下（高维空间）的降维  （eg. t-SNE）</li>
<li>LLE</li>
<li>Laplacian Eigenmaps</li>
<li>t-SNE</li>
</ol>
<h3 id="ML-Lecture16-Unsupervised-Learning-Auto-encoder"><a href="#ML-Lecture16-Unsupervised-Learning-Auto-encoder" class="headerlink" title="ML Lecture16 Unsupervised Learning - Auto-encoder"></a>ML Lecture16 Unsupervised Learning - Auto-encoder</h3><ol>
<li><p>用NN来获得原来数据的compact representation。![image-20190818192309212](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190818192309212.png)</p>
</li>
<li><p>可以用作Text Retrieval、Similar Image Search</p>
</li>
<li><p>用于<strong>Pre-training DNN</strong>–fine tune   20:00左右</p>
<p>用标注好的data预先获得一些w值，然后再用未标注的data进行训练（适用于有大量的<strong>unlabel data</strong>）</p>
</li>
<li><p>Auto-encoder for CNN</p>
</li>
<li><p>可以使用decoder来产生新的data</p>
</li>
</ol>
<h3 id="ML-Lecture17-Unsupervised-Learning-Deep-Generative-Model"><a href="#ML-Lecture17-Unsupervised-Learning-Deep-Generative-Model" class="headerlink" title="???ML Lecture17 Unsupervised Learning - Deep Generative Model"></a>???ML Lecture17 Unsupervised Learning - Deep Generative Model</h3><ol>
<li>Pixel RNN / Variational Autoencoder(VAE) / Generative Adversarial Network(GAN)</li>
<li>PixelRNN : 用LSTM进行训练，对每个像素依次进行训练。</li>
<li>VAE：用autoencoder中的decoder部分进行生成（有类似于控制变量法）。VAE有一些trick</li>
<li><strong>GAN</strong></li>
</ol>
<h3 id="ML-Lecture19-Transfer-Learning"><a href="#ML-Lecture19-Transfer-Learning" class="headerlink" title="???ML Lecture19 Transfer Learning"></a>???ML Lecture19 Transfer Learning</h3><ol>
<li>Data not directly related to the task considered. 如果手中的data不够时，希望用其他的data来进行训练。</li>
</ol>
<p>####Fine-tuning</p>
<ol>
<li><p>One-shot learning – only a few examples in target domain  即target data非常少（相对应的是source data）</p>
</li>
<li><p>Layer Transfer：用source data先train一个model，然后把这个model中的layer拿出来，加上少数layer构成新的model，然后用target data去train新model的新的少数layer。</p>
<p>不同的task，保留的layer是不同的。</p>
</li>
<li><p>只在意target的model表现得好不好，不在意source data。</p>
</li>
</ol>
<p>####Multitask Learning</p>
<p>####![image-20190819193359438](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190819193359438.png)</p>
<h3 id="ML-Lecture20-Support-Vector-Machine-SVM"><a href="#ML-Lecture20-Support-Vector-Machine-SVM" class="headerlink" title="???ML Lecture20 Support Vector Machine(SVM)"></a>???ML Lecture20 Support Vector Machine(SVM)</h3><ol>
<li><p>hinge loss : ![](/Users/xmh_mac/Desktop/屏幕快照 2019-08-19 20.05.45.png)</p>
<p>hinge loss与cross entropy的区别：ce要一直进行下去，会使loss变得越来越小，hl会停在一点，loss=0，结束训练。</p>
</li>
<li><p>Linear SVM : 与Logistic Regression 的区别只在于Loss function – 也可以用Gradient Descent来train。</p>
</li>
<li><p>Kernel Method – Kernel Trick</p>
</li>
</ol>
<h3 id="ML-Lecture21-Recurrent-Neural-Network"><a href="#ML-Lecture21-Recurrent-Neural-Network" class="headerlink" title="ML Lecture21 Recurrent Neural Network"></a>ML Lecture21 Recurrent Neural Network</h3><ol>
<li><p>Elman Network : 每一层的hidden layer的值相对应的传入到下一个时间点的hidden layer</p>
<p>Jordan Network : 上一个时间点的最后的output，传入到下一个时间点的hidden layer（可能表现更好）</p>
</li>
<li><p>Long Short-term Memory(LSTM) :</p>
<p>![image-20190817203107904](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190817203107904.png)</p>
<ul>
<li><p>门的控制函数通常是sigmoid function，通过与0和1的关系来控制门的打开和关闭。</p>
</li>
<li><p>gate function的input在整个NN的input中（与input有不同的transform），gate function的参数在NN被训练的时候学习。![屏幕快照 2019-08-17 下午21.02.03 下午](/Users/xmh_mac/Desktop/屏幕快照 2019-08-17 下午21.02.03 下午.png)</p>
<p>![image-20190817210713496](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190817210713496.png)</p>
</li>
</ul>
</li>
<li><p><strong>为什么使用LSTM来替代RNN？</strong></p>
<ol>
<li><p>Can deal with gradient vanishing(not gradient explode) : RNN的error surface要么很平缓要么狠陡峭–因为同样的w在transition的时候被反复使用，一有变化，就会造成很大影响。LSTM可以解决gradient vanish，但是解决不了gradient explode（仍然会很崎岖）–可以把learning ![image-20190820201213861](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190820201213861.png)rate设置的小一些。LSTM对memory造成的影响，会一直被保存，只会有新的东西加进来，而不会被洗掉，除非forget gate为0。</p>
<p>GRU（Grated Recurrent Unit）：只有两个gate，把input gate和forget gate 进行联动，减少参数数量，可以解决overfitting的问题。</p>
</li>
<li><p><strong>？？？</strong></p>
</li>
</ol>
</li>
<li><p>seq2seq : 机器翻译、语音识别（跨语言）、句法解析</p>
</li>
<li><p>Attention-based Model ：Reading Comprehension</p>
</li>
<li><p><strong>RNN v.s. Structured Learning ？？？？？ 1:10:23左右</strong></p>
</li>
</ol>
<h3 id="ML-Lecture32-Deep-Reinforcement-Learning"><a href="#ML-Lecture32-Deep-Reinforcement-Learning" class="headerlink" title="ML Lecture32 Deep Reinforcement Learning"></a>ML Lecture32 Deep Reinforcement Learning</h3><ol>
<li><p>Reinforcement Learning : Agent learns to take actions to maximize expected reward.</p>
</li>
<li><p>Difficulties of Reinforcement Learning : </p>
<ol>
<li>Reward delay : 不能及时得到反馈。</li>
<li>Agent’s actions affect the subsequent data it receives. Agent要学会去探索他没做过的行为</li>
</ol>
</li>
<li><p>方法： A3C</p>
</li>
<li><p>Policy-based Approach : </p>
<ol>
<li>Learning an Actor : 可以是一个NN（output要是随机的，即不选出概率最高的action执行，而是根据概率随机一个action）</li>
<li>Goodness of Actor : Max很多episode中的total reward的期望</li>
</ol>
<p>![image-20190819214448571](/Users/xmh_mac/Library/Application Support/typora-user-images/image-20190819214448571.png)</p>
<ol start="3">
<li>Gradient Ascent</li>
</ol>
</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Acquaintancy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/11/15/%E6%9D%8E%E5%AE%8F%E6%AF%85ML/">http://yoursite.com/2019/11/15/%E6%9D%8E%E5%AE%8F%E6%AF%85ML/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning    </a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="next-post pull-full"><a href="/2019/11/15/hello-world/"><img class="next_cover lazyload" data-src="https://cdn.jsdelivr.net/gh/jerryc127/CDN@latest/cover/default_bg.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>Hello World</span></div></a></div></nav></div></div><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2019 By Acquaintancy</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>