<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>2019-RoBERTa | ACQUAINT</title><meta name="description" content="2019-RoBERTa"><meta name="keywords" content="Seq2seq-BERT"><meta name="author" content="Acquaintancy"><meta name="copyright" content="Acquaintancy"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="2019-RoBERTa"><meta name="twitter:description" content="2019-RoBERTa"><meta name="twitter:image" content="https://img-blog.csdnimg.cn/20190909163636508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqcDE5MTk=,size_16,color_FFFFFF,t_70"><meta property="og:type" content="article"><meta property="og:title" content="2019-RoBERTa"><meta property="og:url" content="http://yoursite.com/2019/11/17/2019-RoBERTa/"><meta property="og:site_name" content="ACQUAINT"><meta property="og:description" content="2019-RoBERTa"><meta property="og:image" content="https://img-blog.csdnimg.cn/20190909163636508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqcDE5MTk=,size_16,color_FFFFFF,t_70"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://yoursite.com/2019/11/17/2019-RoBERTa/"><link rel="prev" title="Transformer-XL" href="http://yoursite.com/2019/11/17/Transformer-XL/"><link rel="next" title="The Illustrated Transformer" href="http://yoursite.com/2019/11/15/The%20Illustrated%20Transformer/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: undefined,
  medium_zoom: 'false',
  Snackbar: undefined
  
}</script></head><body><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">ACQUAINT</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span><span class="pull_right" id="search_button"></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="https://i.loli.net/2019/11/16/in8qFw425aE6RUx.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">8</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">2</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2019-9-10"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">2019.9.10</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Abstract"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">Abstract</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Introduction"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">Introduction</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Background"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">Background</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Experimental-Setup"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">Experimental Setup</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Implementation"><span class="toc_mobile_items-number">5.0.1.</span> <span class="toc_mobile_items-text">Implementation</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Data"><span class="toc_mobile_items-number">5.0.2.</span> <span class="toc_mobile_items-text">Data</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Evaluation"><span class="toc_mobile_items-number">5.0.3.</span> <span class="toc_mobile_items-text">Evaluation</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Training-Procedure-Analysis"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">Training Procedure Analysis</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Static-vs-Dynamic-Masking"><span class="toc_mobile_items-number">6.0.1.</span> <span class="toc_mobile_items-text">Static vs. Dynamic Masking</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Model-Input-Format-and-Next-Sentence-Prediction"><span class="toc_mobile_items-number">6.0.2.</span> <span class="toc_mobile_items-text">Model Input Format and Next Sentence Prediction</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Training-with-large-batches"><span class="toc_mobile_items-number">6.0.3.</span> <span class="toc_mobile_items-text">Training with large batches</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Text-Encoding"><span class="toc_mobile_items-number">6.0.4.</span> <span class="toc_mobile_items-text">Text Encoding</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#RoBERTa"><span class="toc_mobile_items-number">7.</span> <span class="toc_mobile_items-text">RoBERTa</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Conclusion"><span class="toc_mobile_items-number">8.</span> <span class="toc_mobile_items-text">Conclusion</span></a></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#2019-9-10"><span class="toc-number">1.</span> <span class="toc-text">2019.9.10</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">2.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Introduction"><span class="toc-number">3.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Background"><span class="toc-number">4.</span> <span class="toc-text">Background</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experimental-Setup"><span class="toc-number">5.</span> <span class="toc-text">Experimental Setup</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Implementation"><span class="toc-number">5.0.1.</span> <span class="toc-text">Implementation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Data"><span class="toc-number">5.0.2.</span> <span class="toc-text">Data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Evaluation"><span class="toc-number">5.0.3.</span> <span class="toc-text">Evaluation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-Procedure-Analysis"><span class="toc-number">6.</span> <span class="toc-text">Training Procedure Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Static-vs-Dynamic-Masking"><span class="toc-number">6.0.1.</span> <span class="toc-text">Static vs. Dynamic Masking</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Model-Input-Format-and-Next-Sentence-Prediction"><span class="toc-number">6.0.2.</span> <span class="toc-text">Model Input Format and Next Sentence Prediction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Training-with-large-batches"><span class="toc-number">6.0.3.</span> <span class="toc-text">Training with large batches</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Text-Encoding"><span class="toc-number">6.0.4.</span> <span class="toc-text">Text Encoding</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RoBERTa"><span class="toc-number">7.</span> <span class="toc-text">RoBERTa</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion"><span class="toc-number">8.</span> <span class="toc-text">Conclusion</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://img-blog.csdnimg.cn/20190909163636508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqcDE5MTk=,size_16,color_FFFFFF,t_70)"><div id="post-info"><div id="post-title"><div class="posttitle">2019-RoBERTa</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2019-11-17<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2019-11-17</time><div class="post-meta-wordcount"><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>Post View: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="2019-9-10"><a href="#2019-9-10" class="headerlink" title="2019.9.10"></a>2019.9.10</h2><ol>
<li>宣判了NSP的死刑</li>
<li>更快更高更强 大data 大batch 大sequence</li>
<li>动态mask的效果确实比静态mask好一些</li>
<li>说明BERT还有救 期待XLNet的反击</li>
</ol>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ol>
<li>Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have signiﬁcant impact on the ﬁnal results.</li>
<li>We ﬁnd that <strong>BERT was signiﬁcantly undertrained</strong>, and can match or exceed the performance of every model published after it.</li>
</ol>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ol>
<li>Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances.</li>
<li>We ﬁnd that BERT was signiﬁcantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods.</li>
<li>Our modiﬁcations are simple, they include: <strong>(1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer se-quences; and (4) dynamically changing the masking pattern applied to the training data.</strong></li>
<li>We also collect a large new dataset (CC-N EWS) of comparable size to other privately used datasets, to better control for training set size effects.</li>
<li>In summary, the contributions of this paper are: <strong>(1) We present a set of important BERT design choices and training strategies and introduce alternatives that lead to better downstream task performance; (2) We use a novel dataset, CC-N EWS, and conﬁrm that using more data for pre-training further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods.</strong></li>
</ol>
<ol>
<li><p>训练的计算成本非常大，影响了训练的tuning。并且经常在私人的训练集上训练，不好判断对模型的影响程度。我们还发现BERT被很严重地undertrained，还可以提高性能。</p>
</li>
<li><p>改动有四点：</p>
<ol>
<li>加长模型，加大batch，更多data</li>
<li>直接把nsp丢了</li>
<li>更长的sequence训练</li>
<li>动态mask</li>
<li>还用了一个大一点的数据集以便于控制变量</li>
</ol>
</li>
<li><p>结论就三点：</p>
<ol>
<li>研究了BERT，发现design和train都有提升空间</li>
<li>用了一个新的dataset，并且证明了more data能让downstream tasks更好</li>
<li>我们提升说明了mask这个办法还是很强的，改进改进不比其他最近发的模型差</li>
</ol>
</li>
</ol>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><ol>
<li>The NSP objective was <strong>designed to improve performance on downstream tasks, such as Natural Language Inference</strong>,which require reasoning about the relationships between pairs of sentences.</li>
</ol>
<h2 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h2><h4 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h4><ol>
<li>Primatily follow the original BERT optimization hyperparameters,<strong>except for the peak learning rate and number of warmup steps</strong>, which are tuned separately for each setting.</li>
<li>Additionally found training to be <strong>very sensitive to the Adam epsilon term</strong>, and in some cases we obtained better performance or improved stability after tuning it.</li>
<li>We train only with <strong>full-length sequences.(T=512)</strong></li>
</ol>
<h4 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h4><ol>
<li>BERT-style pretraining crucially relies on <strong>large quantities of text.</strong></li>
</ol>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><p>GLUE/SQuAD/RACE</p>
<h2 id="Training-Procedure-Analysis"><a href="#Training-Procedure-Analysis" class="headerlink" title="Training Procedure Analysis"></a>Training Procedure Analysis</h2><h4 id="Static-vs-Dynamic-Masking"><a href="#Static-vs-Dynamic-Masking" class="headerlink" title="Static vs. Dynamic Masking"></a>Static vs. Dynamic Masking</h4><ol>
<li>The original BERT implementation performed <strong>masking once during data preprocessing</strong>, resulting in a single <strong>static mask</strong>.</li>
<li>To avoid using the same mask for each training instance in every epoch, <strong>training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training</strong>. Thus, each training sequence was <strong>seen with the same mask four times</strong> during training.</li>
<li><strong>Dynamic masking</strong> where we <strong>generate the masking pattern every time</strong> we feed a sequence to the model. This becomes crucial when pretraining <strong>for more steps or with larger datasets</strong>.</li>
</ol>
<ol>
<li>静态mask会导致重复训练语句，并且训练的时候会看到重复的mask</li>
<li>动态mask在每次训练的时候都生成新的mask。</li>
</ol>
<h4 id="Model-Input-Format-and-Next-Sentence-Prediction"><a href="#Model-Input-Format-and-Next-Sentence-Prediction" class="headerlink" title="Model Input Format and Next Sentence Prediction"></a>Model Input Format and Next Sentence Prediction</h4><ol>
<li><p>However, some recent work has questioned the necessity of the NSP loss.</p>
</li>
<li><p>To better understand this discrepancy, we compare several alternative training formats:</p>
<ol>
<li><strong>SEGMENT- PAIR + NSP</strong>:<strong>BERT</strong></li>
<li><strong>SENTENCE - PAIR + NSP</strong>: Each input contains <strong>a pair of natural sentences,</strong> either sampled from a contiguous portion of one document or from separate documents.</li>
<li><strong>FULL - SENTENCES</strong>: Each input is packed with full sentences sampled contiguously from one or more documents,<strong>We remove the NSP loss.</strong></li>
<li><strong>DOC - SENTENCES</strong>: Inputs are constructed similarly to FULL - SENTENCES, except that they may not cross document boundaries.<strong>We remove the NSP loss.</strong></li>
</ol>
</li>
<li><p><strong>We ﬁnd that using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies.</strong> ????</p>
</li>
<li><p><strong>We ﬁnd that this setting outperforms the originally published BERT BASE results and that removing the NSP loss matches or slightly improves downstream task performance.</strong></p>
</li>
<li><p><strong>We ﬁnd that restricting sequences to come from a single document (DOC - SENTENCES) performs slightly better than packing sequences from multiple documents (FULL - SENTENCES).</strong></p>
</li>
</ol>
<ol>
<li>首先 很多工作开始质疑NSP到底有没有用</li>
<li>设计了四种实验 开始对比</li>
<li>单一的句子是不好的，即segment比sentence好。</li>
<li>拿掉NSP，没啥影响甚至效果更好</li>
<li>在一篇文章中选sentence比多篇文章中选sentence效果好，但是因为batchsize不好搞，所以还是用FULL-SENTENCES。</li>
<li>基本宣判了NSP的死刑</li>
</ol>
<h4 id="Training-with-large-batches"><a href="#Training-with-large-batches" class="headerlink" title="Training with large batches"></a>Training with large batches</h4><ol>
<li>We observe that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy.</li>
</ol>
<h4 id="Text-Encoding"><a href="#Text-Encoding" class="headerlink" title="Text Encoding"></a>Text Encoding</h4><ol>
<li>Nev-ertheless, we believe the advantages of a universal encoding scheme outweighs the minor degre-dation in performance and use this encoding in the remainder of our experiments.</li>
</ol>
<h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><ol>
<li>RoBERTa is trained with <strong>dynamic masking</strong> (Section 4.1), <strong>FULL - SENTENCES without NSP loss</strong> (Section 4.2), <strong>large mini-batches</strong> (Section 4.3) and <strong>a larger byte-level BPE</strong> (Section 4.4).</li>
<li>Addition : (1) <strong>the data used for pretraining</strong>, and (2) <strong>the number of training passes through the data</strong>.</li>
<li>We begin by training RoBERTa <strong>following the BERT LARGE architecture.</strong></li>
</ol>
<ol>
<li>用BERT large的结构去训练。</li>
</ol>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ol>
<li>We ﬁnd that performance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer se-quences; and dynamically changing the masking pattern applied to the training data.</li>
</ol>
<ol>
<li>大模型 大batch 大data</li>
<li>拿掉NSP</li>
<li>在更长的句子上训练</li>
<li>动态masking</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Acquaintancy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/11/17/2019-RoBERTa/">http://yoursite.com/2019/11/17/2019-RoBERTa/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Seq2seq-BERT/">Seq2seq-BERT    </a></div><div class="post_share"><div class="social-share" data-image="https://img-blog.csdnimg.cn/20190909163636508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqcDE5MTk=,size_16,color_FFFFFF,t_70" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2019/11/17/Transformer-XL/"><img class="prev_cover lazyload" data-src="https://i.loli.net/2019/11/17/ZGE56rW9TzDyixk.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>Transformer-XL</span></div></a></div><div class="next-post pull_right"><a href="/2019/11/15/The%20Illustrated%20Transformer/"><img class="next_cover lazyload" data-src="https://i.loli.net/2019/11/16/EH9Qb8hDtoWaIkG.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>The Illustrated Transformer</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/11/17/2016-Neural Machine Translation By Jointly Learning To Align And Translate/" title="2016-Neural Machine Translation By Jointly Learning To Align And Translate"><img class="relatedPosts_cover lazyload"data-src="https://i.loli.net/2019/11/17/D6C7o8UM5ZFHbwy.png"><div class="relatedPosts_title">2016-Neural Machine Translation By Jointly Learning To Align And Translate</div></a></div><div class="relatedPosts_item"><a href="/2019/11/17/2017-AttentionIsAllYouNeed/" title="2017-Attention Is All You Need"><img class="relatedPosts_cover lazyload"data-src="https://i.loli.net/2019/11/17/3W2aVmF651n8uAR.png"><div class="relatedPosts_title">2017-Attention Is All You Need</div></a></div><div class="relatedPosts_item"><a href="/2019/11/17/2018-Pre-training of Deep Bidirectional Transformers for Language Understanding/" title="2018-Pre-training of Deep Bidirectional Transformers for Language Understanding"><img class="relatedPosts_cover lazyload"data-src="https://i.loli.net/2019/11/17/bNAr9IDjxsqmXR1.png"><div class="relatedPosts_title">2018-Pre-training of Deep Bidirectional Transformers for Language Understanding</div></a></div><div class="relatedPosts_item"><a href="/2019/11/17/2019-ALBERT/" title="2019-ALBERT"><img class="relatedPosts_cover lazyload"data-src="https://i.loli.net/2019/11/17/wEJFSrsA2pT5KQD.png"><div class="relatedPosts_title">2019-ALBERT</div></a></div><div class="relatedPosts_item"><a href="/2019/11/17/Transformer-XL/" title="Transformer-XL"><img class="relatedPosts_cover lazyload"data-src="https://i.loli.net/2019/11/17/ZGE56rW9TzDyixk.png"><div class="relatedPosts_title">Transformer-XL</div></a></div></div><div class="clear_both"></div></div></div></div><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2019 By Acquaintancy</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>