<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>2017-Attention Is All You Need | ACQUAINT</title><meta name="description" content="2017-Attention Is All You Need"><meta name="keywords" content="Seq2seq-BERT"><meta name="author" content="Acquaintancy"><meta name="copyright" content="Acquaintancy"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="2017-Attention Is All You Need"><meta name="twitter:description" content="2017-Attention Is All You Need"><meta name="twitter:image" content="https://i.loli.net/2019/11/17/3W2aVmF651n8uAR.png"><meta property="og:type" content="article"><meta property="og:title" content="2017-Attention Is All You Need"><meta property="og:url" content="http://yoursite.com/2019/11/17/2017-AttentionIsAllYouNeed/"><meta property="og:site_name" content="ACQUAINT"><meta property="og:description" content="2017-Attention Is All You Need"><meta property="og:image" content="https://i.loli.net/2019/11/17/3W2aVmF651n8uAR.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://yoursite.com/2019/11/17/2017-AttentionIsAllYouNeed/"><link rel="prev" title="2016-Neural Machine Translation By Jointly Learning To Align And Translate" href="http://yoursite.com/2019/11/17/2016-Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20Align%20And%20Translate/"><link rel="next" title="2018-Pre-training of Deep Bidirectional Transformers for Language Understanding" href="http://yoursite.com/2019/11/17/2018-Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: undefined,
  medium_zoom: 'false',
  Snackbar: undefined
  
}</script></head><body><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">ACQUAINT</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span><span class="pull_right" id="search_button"></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="https://i.loli.net/2019/11/16/in8qFw425aE6RUx.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">8</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">2</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">Catalog</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2019-8-22"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">2019.8.22</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Abstract"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">Abstract</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Model-Architecture"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">Model Architecture</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Embeddings-and-Softmax"><span class="toc_mobile_items-number">3.0.1.</span> <span class="toc_mobile_items-text">Embeddings and Softmax</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-4"><a class="toc_mobile_items-link" href="#Position-Encoding"><span class="toc_mobile_items-number">3.0.2.</span> <span class="toc_mobile_items-text">Position Encoding</span></a></li></ol></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#Why-Self-Attention"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">Why Self-Attention</span></a></li></ol></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#2019-8-22"><span class="toc-number">1.</span> <span class="toc-text">2019.8.22</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-number">2.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Architecture"><span class="toc-number">3.</span> <span class="toc-text">Model Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Embeddings-and-Softmax"><span class="toc-number">3.0.1.</span> <span class="toc-text">Embeddings and Softmax</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Position-Encoding"><span class="toc-number">3.0.2.</span> <span class="toc-text">Position Encoding</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-Self-Attention"><span class="toc-number">4.</span> <span class="toc-text">Why Self-Attention</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.loli.net/2019/11/17/3W2aVmF651n8uAR.png)"><div id="post-info"><div id="post-title"><div class="posttitle">2017-Attention Is All You Need</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> Created 2019-11-17<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> Updated 2019-11-17</time><div class="post-meta-wordcount"><i class="fa fa-eye post-meta__icon" aria-hidden="true">       </i><span>Post View: </span><span id="busuanzi_value_page_pv"></span></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><h2 id="2019-8-22"><a href="#2019-8-22" class="headerlink" title="2019.8.22"></a>2019.8.22</h2><ol>
<li><p>Self-attention是个神器，不仅解决了Bi-direction的问题，而且相比于LSTM的好处在于“天涯若比邻”，很长距离的attention也可以通过线性transform来获得。</p>
</li>
<li><p>Multi-head更能使transformer中的每一个input考虑到不同的attention，大大地提高了效果。</p>
</li>
<li><p>可以并行！这个是transformer替代RNN的encoder-decoder的主要因素。也正是这个原因，使得transformer可以用超大规模的data来训练，促使了BERT的产生。</p>
</li>
<li><p>position encoding这个想法很有趣，可以用不同的函数去做。</p>
</li>
<li><p>transformer的encoder–BERT     </p>
<p>transformer的decoder–GPT</p>
</li>
<li><p>注意decoder的结构：输入来自于不同的source–q来自于上一个output，k和v来自于encoder。</p>
</li>
</ol>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ol>
<li>We propose a new simple network architecture,the Transformer,based solely on attention mechanisms,dispensing with recurrence and convolutions entirely.</li>
</ol>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>####Encoder and Decoder Stacks:</p>
<ol>
<li><p><strong>residual connection around each of the two sub-layers ???</strong></p>
</li>
<li><p>Decoder : In addition to the two sub-layers in each encoder layer,the decoder inserts a third sub-layer,which performs multi-head attention over the output of the encoder stack .</p>
</li>
<li><p>Decoder : We also modigy the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.This masking ,combined with fact that the pitput embeddings are offset by one position ,ensures that the predictions for postion i can depend only on the known outputs at positions less that i.</p>
</li>
</ol>
<p>   <em>Encoder由两个层组成，Decoder加了一个mask层，为了保证输出只能参考已经产生的output</em></p>
<p>####Attention</p>
<p><img alt="image-20190822205710351.png" data-src="https://i.loli.net/2019/11/17/3W2aVmF651n8uAR.png" class="lazyload"></p>
<ol>
<li><p><strong>Scaled Dot-Product Attention</strong> : We call our particular attention “Scaled Dot-Product Attention”. The two most commonly used attention functions are <strong>additive attention,and dot-product attention</strong> .Dot-product attention is identical to our algorithm .Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.While the two are similar in theoretical complexity,<strong>dot-product attention is much faster and more space-efficient in practice,since it can be implemented using highly optimized matrix multiplication code</strong>.</p>
<p>While for small values of dk (dimention of vectors) the two mechanisms perform similarly, <strong>additive attention outperforms dot product attention without scaling for larger values of dk</strong>. We suspect that for large values of dk , the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients .</p>
</li>
<li><p>Multi-Head Attention : Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.(one head–attend on one word  /   multi-head–attend on different words)</p>
<p>####!!!!!</p>
<p><strong>Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.(8 heads – for each heads we use d=64=512/8)</strong></p>
<p><strong>CONCAT!!!</strong></p>
<p><img alt="image-20190822213507666.png" data-src="https://i.loli.net/2019/11/17/uicMjlLCpA4N1EJ.png" class="lazyload"></p>
</li>
<li><p>Applications of Attention in our Model :</p>
<ol>
<li>In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.</li>
<li>In a self-attention layer all of the keys, values and queries come from the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.</li>
<li>Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information ﬂow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.</li>
</ol>
</li>
</ol>
<ol>
<li><p><em>除了做点积之外，还可以用单层的前向神经网络计算Attention。但是点积更快也更有空间效率（因为矩阵）。用NN的方法在高维输入上表现会更好，但是除以平方根的方法让点积的方法的表现也提升到了差不多的水平。</em></p>
</li>
<li><p><em>One-head Attention每个position只能选择一个（一种）attention，只有一种选择。Multi-head可以让一个position有多种选择，选择多个attention去attend。</em></p>
<p><em>Multi-head 的输入时把One-head的输入切开的，即原来512维，用8个head，每个head的维数就是64维。这也是为什么在输出上，可以把每个head的输出做拼接（concat）成512维，然后乘以矩阵作为output。</em></p>
</li>
<li><p><strong><em>在decoder中，attention的输入q是来源于上一层attention的，k和v是来源于encoder的。这个很多文章没有讲到。</em></strong></p>
<p><em>encoder中，attention的每个输入都来自于上一层，因为上一层的每个output都考虑了每个position。</em></p>
<p><em>同理，decoder也是来自于每个position，但是有一些特殊的输入我们不想考虑，则设为负无穷，这样通过softmax就可以将他的权重置为0。</em></p>
</li>
</ol>
<p>####Position-wise Feed-Forward Networks</p>
<ol>
<li>A fully connected FFN,which is applied to each position separately and identically.<strong>This consists of two linear transformations with a ReLU activation in between .</strong>While the linear transformations are the same across different positions, <strong>they use different parameters from layer to layer</strong>.</li>
</ol>
<p><img alt="image-20190822215814556.png" data-src="https://i.loli.net/2019/11/17/RJjmnC1wp5AxB9L.png" class="lazyload"></p>
<ol>
<li><em>FFN中，两次线性变换，在两次之间有一个ReLU激活函数。每一层的每个position的FFN是相同参数的，但是层与层之间的FFN是不同参数的，跟随model一起train出来。</em></li>
</ol>
<h4 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h4><ol>
<li>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax √ linear transformation.</li>
</ol>
<h4 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h4><ol>
<li><p>The positional encodings have <strong>the same dimension d as the embeddings, so that the two can be summed</strong>. There are many choices of positional encodings, learned and ﬁxed .</p>
</li>
<li><p>In this work, we use sine and cosine functions of different frequencies.</p>
<p><img alt="image-20190822220314347.png" data-src="https://i.loli.net/2019/11/17/wYURkIXZKhg3aFm.png" class="lazyload"></p>
</li>
<li><p>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions . We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</p>
</li>
</ol>
<ol>
<li><em>Position Encoding的维度和输入维度相同，所以可以直接相加。（原理可以想成在原始input的vector是拼出来的，然后经过线性变换之后，拼接变加法）</em></li>
<li><em>选用sin和cos作为参数（原理另查）</em></li>
</ol>
<h2 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h2><ol>
<li><p>Motivating our use of self-attention we consider three desiderata.</p>
<p>One is the <strong>total computational complexity</strong> per layer. Another is the amount of computation that can be <strong>parallelized</strong>, as measured by the minimum number of sequential operations required.The third is <strong>the path length between long-range dependencies</strong> in the network.</p>
</li>
<li><p>The Path Length : a self-attention layer connects all positions with a constant number of sequentially executed operations .</p>
</li>
<li><p>Computational complexity : <strong>self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d</strong>, which is most often the case with sentence representations used by state-of-the-art models in machine translations.</p>
</li>
</ol>
<ol>
<li><em>三个理由：减少每一层的运算复杂度、可以并行、长距离的词语可以用很短的path去连接。</em></li>
<li><em>Path Length：基于self-attention机制，远距离的词语可以直接连接，所以path更短。</em></li>
<li><em>计算复杂度上，在序列长度小于输入维数时，Self-attention更好（这种情况更为常见）。</em></li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Acquaintancy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2019/11/17/2017-AttentionIsAllYouNeed/">http://yoursite.com/2019/11/17/2017-AttentionIsAllYouNeed/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Seq2seq-BERT/">Seq2seq-BERT    </a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2019/11/17/3W2aVmF651n8uAR.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button"><i class="fa fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.jpg"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.jpg"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2019/11/17/2016-Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20Align%20And%20Translate/"><img class="prev_cover lazyload" data-src="https://i.loli.net/2019/11/17/D6C7o8UM5ZFHbwy.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Previous Post</div><div class="prev_info"><span>2016-Neural Machine Translation By Jointly Learning To Align And Translate</span></div></a></div><div class="next-post pull_right"><a href="/2019/11/17/2018-Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding/"><img class="next_cover lazyload" data-src="https://i.loli.net/2019/11/17/bNAr9IDjxsqmXR1.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">Next Post</div><div class="next_info"><span>2018-Pre-training of Deep Bidirectional Transformers for Language Understanding</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> Recommend</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/11/17/2016-Neural Machine Translation By Jointly Learning To Align And Translate/" title="2016-Neural Machine Translation By Jointly Learning To Align And Translate"><img class="relatedPosts_cover lazyload"data-src="https://i.loli.net/2019/11/17/D6C7o8UM5ZFHbwy.png"><div class="relatedPosts_title">2016-Neural Machine Translation By Jointly Learning To Align And Translate</div></a></div><div class="relatedPosts_item"><a href="/2019/11/17/2018-Pre-training of Deep Bidirectional Transformers for Language Understanding/" title="2018-Pre-training of Deep Bidirectional Transformers for Language Understanding"><img class="relatedPosts_cover lazyload"data-src="https://i.loli.net/2019/11/17/bNAr9IDjxsqmXR1.png"><div class="relatedPosts_title">2018-Pre-training of Deep Bidirectional Transformers for Language Understanding</div></a></div><div class="relatedPosts_item"><a href="/2019/11/17/2019-ALBERT/" title="2019-ALBERT"><img class="relatedPosts_cover lazyload"data-src="https://i.loli.net/2019/11/17/wEJFSrsA2pT5KQD.png"><div class="relatedPosts_title">2019-ALBERT</div></a></div><div class="relatedPosts_item"><a href="/2019/11/17/Transformer-XL/" title="Transformer-XL"><img class="relatedPosts_cover lazyload"data-src="https://i.loli.net/2019/11/17/ZGE56rW9TzDyixk.png"><div class="relatedPosts_title">Transformer-XL</div></a></div><div class="relatedPosts_item"><a href="/2019/11/17/2019-RoBERTa/" title="2019-RoBERTa"><img class="relatedPosts_cover lazyload"data-src="https://img-blog.csdnimg.cn/20190909163636508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqcDE5MTk=,size_16,color_FFFFFF,t_70"><div class="relatedPosts_title">2019-RoBERTa</div></a></div></div><div class="clear_both"></div></div></div></div><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2019 By Acquaintancy</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="Read Mode"></i><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-list-ul close" id="mobile-toc-button" title="Table of Contents" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>