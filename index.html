<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>ACQUAINT</title><meta name="description" content=""><meta name="keywords"><meta name="author" content="Acquaintancy"><meta name="copyright" content="Acquaintancy"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="ACQUAINT"><meta name="twitter:description" content=""><meta name="twitter:image" content="https://i.loli.net/2019/11/16/in8qFw425aE6RUx.jpg"><meta property="og:type" content="website"><meta property="og:title" content="ACQUAINT"><meta property="og:url" content="http://yoursite.com/"><meta property="og:site_name" content="ACQUAINT"><meta property="og:description" content=""><meta property="og:image" content="https://i.loli.net/2019/11/16/in8qFw425aE6RUx.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://yoursite.com/"><link rel="preload" href="https://i.loli.net/2019/11/16/gkjJ8lGBQY7NL1h.jpg" as="image"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  highlight_lang: 'true',
  highlight_shrink: 'false',
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  copyright: undefined,
  copy_copyright_js: false,
  ClickShowText: undefined,
  medium_zoom: 'false',
  Snackbar: undefined
  
}</script></head><body><div id="header"> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">ACQUAINT</a></span><i class="fa fa-bars fa-fw toggle-menu pull_right close" aria-hidden="true"></i><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></span><span class="pull_right" id="search_button"></span></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="lazyload avatar_img" src="https://i.loli.net/2019/11/16/in8qFw425aE6RUx.jpg" onerror="onerror=null;src='/img/friend_404.gif'"></div><div class="mobile_post_data"><div class="mobile_data_item is_center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">8</div></a></div></div><div class="mobile_data_item is_center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">2</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a></div></div></div></div><div id="body-wrap"><nav class="full_page" id="nav"><div class="nav_bg" style="background-image: url(https://i.loli.net/2019/11/16/gkjJ8lGBQY7NL1h.jpg)"></div><div id="site-info"><div id="site-title"><span class="blogtitle">ACQUAINT</span></div><div id="site-sub-title"><span class="subtitle"></span></div><div id="site-social-icons"><a class="social-icon" href="https://github.com/acquaintancy-xmh" target="_blank"><i class="fa fa-github"></i></a><a class="social-icon" href="mailto:lg.673@163.com" target="_blank"><i class="fa fa-envelope"></i></a></div></div><div class="scroll-down"><i class="fa fa-angle-down scroll-down-effects"></i></div></nav><div id="content-outer"><div class="layout_page" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item article-container"><div class="post_cover pull_left left_radius"><a href="/2019/11/17/2016-Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20Align%20And%20Translate/" title="2016-Neural Machine Translation By Jointly Learning To Align And Translate"><img class="post_bg lazyload" data-src="https://i.loli.net/2019/11/17/D6C7o8UM5ZFHbwy.png" onerror="onerror=null;src='/img/404.jpg'"></a></div><div class="recent-post-info">  <a class="article-title" href="/2019/11/17/2016-Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20Align%20And%20Translate/" title="2016-Neural Machine Translation By Jointly Learning To Align And Translate">2016-Neural Machine Translation By Jointly Learning To Align And Translate</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-11-17</time><div class="content">2019.8.25
最开始设计attention是为了解决encoder-decoder对长句子不友好的问题。

这里的attention（也就是alignment model 也就是标题的来源）是用decoder中hidden layer的state和encoder中hidden layer的st ...</div></div></div><div class="recent-post-item article-container"><div class="post_cover pull_right right_radius"><a href="/2019/11/17/2017-AttentionIsAllYouNeed/" title="2017-Attention Is All You Need"><img class="post_bg lazyload" data-src="https://i.loli.net/2019/11/17/3W2aVmF651n8uAR.png" onerror="onerror=null;src='/img/404.jpg'"></a></div><div class="recent-post-info">  <a class="article-title" href="/2019/11/17/2017-AttentionIsAllYouNeed/" title="2017-Attention Is All You Need">2017-Attention Is All You Need</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-11-17</time><div class="content">2019.8.22
Self-attention是个神器，不仅解决了Bi-direction的问题，而且相比于LSTM的好处在于“天涯若比邻”，很长距离的attention也可以通过线性transform来获得。

Multi-head更能使transformer中的每一个input考虑到不同的at ...</div></div></div><div class="recent-post-item article-container"><div class="post_cover pull_left left_radius"><a href="/2019/11/17/2018-Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding/" title="2018-Pre-training of Deep Bidirectional Transformers for Language Understanding"><img class="post_bg lazyload" data-src="https://i.loli.net/2019/11/17/bNAr9IDjxsqmXR1.png" onerror="onerror=null;src='/img/404.jpg'"></a></div><div class="recent-post-info">  <a class="article-title" href="/2019/11/17/2018-Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding/" title="2018-Pre-training of Deep Bidirectional Transformers for Language Understanding">2018-Pre-training of Deep Bidirectional Transformers for Language Understanding</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-11-17</time><div class="content">2019.10.16
在论文中，BERT的输入之一-Segment Embedding 是训练出来的。
因为fine-tuning的时候要将BERT与下游模型共同训练，所以原则上要保证目标函数的一致性。但是MLM在下游模型中不出现，故取80-10-10的分配。
The advantage of th ...</div></div></div><div class="recent-post-item article-container"><div class="post_cover pull_right right_radius"><a href="/2019/11/17/2019-ALBERT/" title="2019-ALBERT"><img class="post_bg lazyload" data-src="https://i.loli.net/2019/11/17/wEJFSrsA2pT5KQD.png" onerror="onerror=null;src='/img/404.jpg'"></a></div><div class="recent-post-info">  <a class="article-title" href="/2019/11/17/2019-ALBERT/" title="2019-ALBERT">2019-ALBERT</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-11-17</time><div class="content">Abstract
更大的模型通常会提升性能，但是也会受到GPU的限制、需要更长的训练时间和可以造成的模型degradation。
提出了两种parameter-reduction技术，降低计算复杂度提升BERT的训练速度。
用self-supervised loss去专注于建模句子间的连贯性，并表明 ...</div></div></div><div class="recent-post-item article-container"><div class="post_cover pull_left left_radius"><a href="/2019/11/17/Transformer-XL/" title="Transformer-XL"><img class="post_bg lazyload" data-src="https://i.loli.net/2019/11/17/ZGE56rW9TzDyixk.png" onerror="onerror=null;src='/img/404.jpg'"></a></div><div class="recent-post-info">  <a class="article-title" href="/2019/11/17/Transformer-XL/" title="Transformer-XL">Transformer-XL</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-11-17</time><div class="content">
原始Transformer有一个劣势是在语言模型的设置上只能输入固定长度的内容（limited by a fixed-length context）
细想一下，BERT在应用Transformer时，有一个参数sequence length，也就是BERT在训练和预测时，每次接受的输入是固定长度的 ...</div></div></div><div class="recent-post-item article-container"><div class="post_cover pull_right right_radius"><a href="/2019/11/17/2019-RoBERTa/" title="2019-RoBERTa"><img class="post_bg lazyload" data-src="https://img-blog.csdnimg.cn/20190909163636508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xqcDE5MTk=,size_16,color_FFFFFF,t_70" onerror="onerror=null;src='/img/404.jpg'"></a></div><div class="recent-post-info">  <a class="article-title" href="/2019/11/17/2019-RoBERTa/" title="2019-RoBERTa">2019-RoBERTa</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-11-17</time><div class="content">2019.9.10
宣判了NSP的死刑
更快更高更强 大data 大batch 大sequence
动态mask的效果确实比静态mask好一些
说明BERT还有救 期待XLNet的反击

Abstract
Training is computationally expensive, often do ...</div></div></div><div class="recent-post-item article-container"><div class="post_cover pull_left left_radius"><a href="/2019/11/15/The%20Illustrated%20Transformer/" title="The Illustrated Transformer"><img class="post_bg lazyload" data-src="https://i.loli.net/2019/11/16/EH9Qb8hDtoWaIkG.png" onerror="onerror=null;src='/img/404.jpg'"></a></div><div class="recent-post-info">  <a class="article-title" href="/2019/11/15/The%20Illustrated%20Transformer/" title="The Illustrated Transformer">The Illustrated Transformer</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-11-15</time><div class="content">Self-Attention 翻译，from Jay Alammar
https://jalammar.github.io/illustrated-transformer/
2019.11.15更新完成了encoding的部分，还剩下position embedding和decoding有机会日后完 ...</div></div></div><div class="recent-post-item article-container"><div class="post_cover pull_right right_radius"><a href="/2019/08/25/Conclusion%20Seq2Seq-Attention-Transformer-BERT/" title="Conclusion Seq2Seq-Attention-Transformer-BERT"><img class="post_bg lazyload" data-src="https://i.loli.net/2019/11/16/gkjJ8lGBQY7NL1h.jpg" onerror="onerror=null;src='/img/404.jpg'"></a></div><div class="recent-post-info">  <a class="article-title" href="/2019/08/25/Conclusion%20Seq2Seq-Attention-Transformer-BERT/" title="Conclusion Seq2Seq-Attention-Transformer-BERT">Conclusion Seq2Seq-Attention-Transformer-BERT</a><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-08-25</time><div class="content">Seq2seq to Attention
目的：
从encoder中提取出的fixed-length vector是限制model的瓶颈（长句子不友好）

改变：
encoder中的RNN用Bi-directional RNN代替
加入attention机制


Attention to Trans ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span></div></nav></div><div class="aside_content" id="aside_content"><div class="card_widget card-author"><div class="card-content"><div class="post_data"><div class="data-item is_center"><img class="lazyload avatar_img" src="https://i.loli.net/2019/11/16/in8qFw425aE6RUx.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><p class="author-info__name is_center">Acquaintancy</p><p class="author-info__description is_center"></p></div></div><div class="post_data data_config"><div class="data-item is_center"><div class="data_link"><a href="/archives/"><p class="headline">Articles</p><p class="length_num">8</p></a></div></div><div class="data-item is_center">      <div class="data_link"><a href="/tags/"><p class="headline">Tags</p><p class="length_num">2</p></a></div></div></div><div class="post_data is_center"><a class="data-item bookmark bookmarke--primary bookmark--animated" id="bookmark-it" href="javascript:;" title="Add to bookmark" target="_self"><i class="fa fa-bookmark" aria-hidden="true"></i><span>Add to bookmark</span></a></div><div class="post_data data_config"><div id="aside-social-icons"> <a class="social-icon data-item" href="https://github.com/acquaintancy-xmh" target="_blank"><i class="fa fa-github"></i></a><a class="social-icon data-item" href="mailto:lg.673@163.com" target="_blank"><i class="fa fa-envelope"></i></a></div></div></div></div><div class="card_widget card-announcement"><div class="card-content"><div class="item_headline"><i class="fa fa-bullhorn card-announcement-animation" aria-hidden="true"></i><span>Announcement</span></div><div class="announcement_content">感謝訪問本站，若喜歡請收藏 ^_^</div></div></div><div class="card_widget card-recent-post"><div class="card-content"><div class="item_headline"><i class="fa fa-history" aria-hidden="true"></i><span>Recent Post</span></div><div class="aside_recent_item"><div class="aside_recent_post"><a href="/2019/11/17/2016-Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20Align%20And%20Translate/"><div class="aside_post_cover"><img class="aside_post_bg lazyload" data-src="https://i.loli.net/2019/11/17/D6C7o8UM5ZFHbwy.png" onerror="onerror=null;src='/img/404.jpg'" title="2016-Neural Machine Translation By Jointly Learning To Align And Translate"></div><div id="aside_title"><div class="aside_post_title" href="/2019/11/17/2016-Neural%20Machine%20Translation%20By%20Jointly%20Learning%20To%20Align%20And%20Translate/" title="2016-Neural Machine Translation By Jointly Learning To Align And Translate">2016-Neural Machine Translation By Jointly Learning To Align And Translate</div><time class="aside_post_meta post-meta__date">2019-11-17</time></div></a></div><div class="aside_recent_post"><a href="/2019/11/17/2017-AttentionIsAllYouNeed/"><div class="aside_post_cover"><img class="aside_post_bg lazyload" data-src="https://i.loli.net/2019/11/17/3W2aVmF651n8uAR.png" onerror="onerror=null;src='/img/404.jpg'" title="2017-Attention Is All You Need"></div><div id="aside_title"><div class="aside_post_title" href="/2019/11/17/2017-AttentionIsAllYouNeed/" title="2017-Attention Is All You Need">2017-Attention Is All You Need</div><time class="aside_post_meta post-meta__date">2019-11-17</time></div></a></div><div class="aside_recent_post"><a href="/2019/11/17/2018-Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding/"><div class="aside_post_cover"><img class="aside_post_bg lazyload" data-src="https://i.loli.net/2019/11/17/bNAr9IDjxsqmXR1.png" onerror="onerror=null;src='/img/404.jpg'" title="2018-Pre-training of Deep Bidirectional Transformers for Language Understanding"></div><div id="aside_title"><div class="aside_post_title" href="/2019/11/17/2018-Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding/" title="2018-Pre-training of Deep Bidirectional Transformers for Language Understanding">2018-Pre-training of Deep Bidirectional Transformers for Language Understanding</div><time class="aside_post_meta post-meta__date">2019-11-17</time></div></a></div><div class="aside_recent_post"><a href="/2019/11/17/2019-ALBERT/"><div class="aside_post_cover"><img class="aside_post_bg lazyload" data-src="https://i.loli.net/2019/11/17/wEJFSrsA2pT5KQD.png" onerror="onerror=null;src='/img/404.jpg'" title="2019-ALBERT"></div><div id="aside_title"><div class="aside_post_title" href="/2019/11/17/2019-ALBERT/" title="2019-ALBERT">2019-ALBERT</div><time class="aside_post_meta post-meta__date">2019-11-17</time></div></a></div><div class="aside_recent_post"><a href="/2019/11/17/Transformer-XL/"><div class="aside_post_cover"><img class="aside_post_bg lazyload" data-src="https://i.loli.net/2019/11/17/ZGE56rW9TzDyixk.png" onerror="onerror=null;src='/img/404.jpg'" title="Transformer-XL"></div><div id="aside_title"><div class="aside_post_title" href="/2019/11/17/Transformer-XL/" title="Transformer-XL">Transformer-XL</div><time class="aside_post_meta post-meta__date">2019-11-17</time></div></a></div></div></div></div><div class="card_widget card-tags"><div class="card-content"><div class="item_headline"><i class="fa fa-tags" aria-hidden="true"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/Self-attention-%E7%BF%BB%E8%AF%91/" style="font-size: 16px; color: #999">Self-attention 翻译</a> <a href="/tags/Seq2seq-BERT/" style="font-size: 24px; color: #000">Seq2seq-BERT</a></div></div></div><div class="card_widget card-archives"><div class="card-content"><div class="item_headline"><i class="fa fa-archive" aria-hidden="true"></i><span>Archives</span></div><div class="archives_item"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">2019年11月<span class="archive-list-count">7</span></a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">2019年08月<span class="archive-list-count">1</span></a></li></ul></div></div></div><div class="card_widget card-webinfo"><div class="card-content"><div class="item_headline"><i class="fa fa-line-chart" aria-hidden="true"></i><span>Info</span></div><div class="webinfo"><div class="webinfo_item"><div class="webinfo_article_name">Article :</div><div class="webinfo_article_count">8</div></div><div class="webinfo_item"><div class="webinfo_runtime_name">Run time :</div><div class="webinfo_runtime_count" id="webinfo_runtime_count"></div><script id="runtionshow" src="/js/runtimeshow.js" start_date="11/14/2019 00:00:00">      </script></div><div class="webinfo_item">      <div class="webinfo_site_uv_name">UV :</div><div class="webinfo_site_uv_count" id="busuanzi_value_site_uv"></div></div><div class="webinfo_item"><div class="webinfo_site_name">PV :</div><div class="webinfo_site_pv_count" id="busuanzi_value_site_pv"></div></div></div></div></div></div></div></div><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2018 - 2019 By Acquaintancy</div><div class="framework-info"><span>Driven </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-plus" id="font_plus" title="Increase font size"></i><i class="fa fa-minus" id="font_minus" title="Decrease font size"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="Traditional Chinese and Simplified Chinese Conversion" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="Dark Mode"></i></div><div id="rightside-config-show"><div id="rightside_config" title="Setting"><i class="fa fa-cog" aria-hidden="true"></i></div><i class="fa fa-arrow-up" id="go-up" title="Back to top" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()
</script><script>if (/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {} else {
  $('.full_page .nav_bg').css('background-attachment', 'fixed');
}

</script><script src="https://cdn.jsdelivr.net/npm/typed.js"></script><script>var typed = new Typed(".subtitle", {
strings: 'I Promise'.split(","),
startDelay: 300,
typeSpeed: 100,
loop: true,
backSpeed: 50
});
</script><script>//首頁fullpage
function alignContent() {
  for (var n = $(window).height(), e = document.querySelectorAll(".full_page"), i = 0; i < e.length; i++) 
    e[i].style.height = n + "px";
    $("#site-info").each(function () {
      var x = $(this).height();
      $(this).css("top", (n-x)/2)       
    })
}
alignContent();

$(window).bind("resize", function () {
  alignContent()
})</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script></body></html>